```markdown
# ğŸŒ Company Insight Scraper â€“ Backend

This project is a **backend web scraping service** that accepts either:

- A list of **company websites**  
- A **search query** (e.g., _"cloud computing startups in Europe"_)

...and extracts the following company details using **Selenium**:

- âœ… Company name (from homepage or contact/about page)
- ğŸ“§ Email addresses
- â˜ï¸ Phone numbers
- ğŸ”— Contact/About page URL

All results are saved to a **MongoDB** collection (`scraper_db > companies`).

---

## ğŸ›  Features

- ğŸ” **Query-based Discovery**  
  Automatically discovers company websites from a search query using Bing.

- ğŸ•·ï¸ **Selenium-based Contact Scraping**  
  Navigates company websites to intelligently find `Contact` or `About` pages and scrape key info.

- ğŸ§  **Smart Extraction**  
  Uses regex and BeautifulSoup to extract emails and phone numbers.

- ğŸ“¦ **MongoDB Integration**  
  Stores each company's full metadata in a MongoDB collection.

---

## ğŸ“ Project Structure

```

company-insights/
â”œâ”€â”€ main.py                 # FastAPI app
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes.py           # API endpoints
â”œâ”€â”€ scraper/
â”‚   â””â”€â”€ selenium\_scraper.py # Core scraping logic
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ search.py           # Query-to-URLs logic using Bing
â”œâ”€â”€ db/
â”‚   â””â”€â”€ mongo\_client.py     # MongoDB connection + save logic
â”œâ”€â”€ .env                    # MongoDB URI (MONGO\_URI=...)
â””â”€â”€ requirements.txt        # Python dependencies

````

---

## ğŸš€ How to Run

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/company-insights.git
cd company-insights
````

### 2. Setup Python Environment

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

### 3. Setup `.env`

Create a `.env` file in the root with your MongoDB URI:

```
MONGO_URI=mongodb://localhost:27017
```

### 4. Run the Server

```bash
uvicorn main:app --reload
```

---

## ğŸ§ª Sample API Usage

### â¤ Search Query Endpoint

```bash
curl -X POST http://127.0.0.1:8000/api/scrape-query \
  -H 'Content-Type: application/json' \
  -d '{"query": "five cloud computing companies in Europe"}'
```

### â¤ Direct URL List (for future use)

```bash
curl -X POST http://127.0.0.1:8000/api/scrape-urls \
  -H 'Content-Type: application/json' \
  -d '{"urls": ["https://example.com", "https://another.com"]}'
```

---

## ğŸ“¥ MongoDB Output Example

```json
{
  "_id": "ObjectId(...)",
  "name": "OneLab Ventures",
  "url": "https://www.onelabventures.com/",
  "contact_page": "https://www.onelabventures.com/about-us",
  "emails": ["contact@onelabventures.com", "jdutta@onelabventures.com"],
  "phones": ["+91 882 713 5321", "+1 978 464 1930"]
}
```

---

## ğŸ§¾ Notes

* Frontend is **not implemented** yet â€“ only backend is available.
* Headless mode is off in Selenium for easier debugging. It can be enabled later for production.
* Avoid scraping too frequently to respect website rate limits.

---

## ğŸ“Œ TODO (Upcoming)

* [ ] Frontend React interface for entering queries & displaying results
* [ ] Async scraping status page
* [ ] Domain filtering and deduplication
* [ ] Proxy/rotating user agents

---

## ğŸ‘¨â€ğŸ’» Author

**Karthik Dileep**
*Developed as part of a backend engineering assessment*

---

## ğŸ“„ License

This project is for educational and demonstration purposes only.

```
